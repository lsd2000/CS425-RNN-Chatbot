{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoQ4_TlXybJ9"
      },
      "source": [
        "###Importing Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mVkBeAG-dTPF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-iJx8HwyhLr"
      },
      "source": [
        "###Reading the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RsJ7lLK1Cay_"
      },
      "outputs": [],
      "source": [
        "file = open('./data/Dataset.txt','r').read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sSNomuT7Jsvv"
      },
      "outputs": [],
      "source": [
        "raw_data = [f.split('\\t') for f in file.split('\\n')]    #separating questions and answers\n",
        "questions = [x[0] for x in raw_data] # questions for the first section\n",
        "answers = [x[1] if len(x) > 1 else \"\" for x in raw_data] #answers for the second section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiA-0GwtJwYd",
        "outputId": "dddbbb76-4209-402a-e39e-fc296d863f8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  hi, how are you doing?\n",
            "Answer:  i'm fine. how about yourself?\n"
          ]
        }
      ],
      "source": [
        "print(\"Question: \", questions[0])\n",
        "print(\"Answer: \", answers[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFuAVvFuyo64"
      },
      "source": [
        "###Tokenizing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WXM48uqPI24W"
      },
      "outputs": [],
      "source": [
        "#Function convert text into numerical representation suitable for machine learning for working with neural networks in tensorflow\n",
        "def tokenize(lang): #lang is the text sentences in a particular language\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='') #tokenizer created responsible to convert them into numerical tokens\n",
        "    lang_tokenizer.fit_on_texts(lang) # learn the vocabulary of the language\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang) #convert input text into sequences of numerical tokens\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post') #ensure sequences have same length, post to match longest sequence\n",
        "    return tensor, lang_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P57M5HwNyrtM"
      },
      "source": [
        "###PreProcessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qtQWA702Iwxx"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    #normalize the sentence by decomposing \n",
        "    #any composed characters into their basic components.  eg cafe with the ' on top of e\n",
        "    #For example, it converts accented characters into their non-accented counterparts.\n",
        "    sentence = ''.join(c for c in unicodedata.normalize('NFD', sentence) if unicodedata.category(c) != 'Mn')  #exclude combining diacritical marks. This step removes diacritics from characters. (symbols below)\n",
        "    sentence = sentence.lower().strip() #converts the sentence to lowercase and removes leading and trailing whitespace.\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) #adds spaces around punctuation marks (such as '.', '?', '!', ',') to separate them from words.\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) #replaces consecutive spaces with a single space.\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)# replaces any characters that are not letters, punctuation marks, or whitespace with a space.\n",
        "    sentence = '<start> ' + sentence + ' <end>' #adds special tokens <start> and <end> to the sentence. These tokens can be used to indicate the start and end of a sequence during model training.\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ubN0jC6zJ0hH"
      },
      "outputs": [],
      "source": [
        "pre_questions = [preprocess_sentence(w) for w in questions] #processing all the questions\n",
        "pre_answers = [preprocess_sentence(w) for w in answers] #processing all the answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EsOXfre3KSDy"
      },
      "outputs": [],
      "source": [
        "data = pre_answers, pre_questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ryMnanp0KBWw"
      },
      "outputs": [],
      "source": [
        "    #tuple data consisting of two elements: targ_lang and inp_lang. \n",
        "    \n",
        "def prepare_data(data): \n",
        "\n",
        "    targ_lang, inp_lang = data #These are presumably the target language (answer) and the input language (question).\n",
        "\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang) #tokenize function from earlier\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang) #tokenize function from earlier\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
        "\n",
        "# input_tensor: Tokenized and padded representations of the input sequences.\n",
        "# target_tensor: Tokenized and padded representations of the target sequences.\n",
        "# inp_lang_tokenizer: The tokenizer for the input language.\n",
        "# targ_lang_tokenizer: The tokenizer for the target language.\n",
        "\n",
        "#Inverse Mapping: Tokenizers also facilitate the inverse mapping, \n",
        "# i.e., converting numerical IDs back to text. This is useful for generating human-readable responses or interpreting model outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xJV_dqtyKYzm"
      },
      "outputs": [],
      "source": [
        "input_tensor, target_tensor, inp_lang, targ_lang = prepare_data(data) #Calling the above function\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
        "#retrieve the sequence lengths from the dimensions of the tensors. \n",
        "# The maximum sequence length is important for model training, as it determines the length of the sequences that the model can handle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQAe2JYuJycK",
        "outputId": "d06bcf15-929a-4fd4-b941-6b8b49a590a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24\n",
            "24\n"
          ]
        }
      ],
      "source": [
        "print(max_length_targ)\n",
        "print(max_length_inp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7uEiPyo7DyI"
      },
      "source": [
        "### Downloading the Tokenizers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Fjor8_8K6__v"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "#uses it to save the tokenizers for the input and target languages as binary files \n",
        "# using the pickle library. Here's what each part of the code does:\n",
        "def save_tokenizer(tokenizer, filename): #tokenizer is the tokenizer object that you want to save. filename is the name of the file where the tokenizer will be saved.\n",
        "    with open(filename, 'wb') as handle: #()'wb'), which is suitable for saving binary data.\n",
        "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "#specifies that the highest available protocol should be used for pickling. \n",
        "# This can be helpful for ensuring compatibility with future Python versions.\n",
        "save_tokenizer(inp_lang, 'input_tokenizer.pkl')\n",
        "save_tokenizer(targ_lang, 'target_tokenizer.pkl')\n",
        "\n",
        "#save the tokenizers to disk as binary files. \n",
        "# Saving tokenizers is useful when you want to reuse them for processing new data in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqptaUlQyw5m"
      },
      "source": [
        "###Splitting the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXEJ-kkDWDIH",
        "outputId": "a318fd4c-bb00-4f08-c4d1-5f3f9f0db71d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "# The function returns four variables:\n",
        "# input_tensor_train: The input sequences for the training set.\n",
        "# input_tensor_val: The input sequences for the validation set.\n",
        "# target_tensor_train: The target sequences for the training set.\n",
        "# target_tensor_val: The target sequences for the validation set.\n",
        "# Splitting the data into 90% train, 10% validation\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(\n",
        "    input_tensor, target_tensor, test_size=0.1, random_state=42)\n",
        "#input_tensor and target_tensor are the data to be split. input_tensor likely contains the input sequences, \n",
        "# and target_tensor contains the corresponding target sequences (answers or responses).\n",
        "#test_size is set to 0.1, indicating that 10% of the data will be used for validation, and the remaining 90% for training.\n",
        "#random_state is set to 42, which provides a seed for the random number generator. \n",
        "#This ensures reproducibility when splitting the data. Using the same seed will result in the same split every time the code is run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3nQQ917yzKF"
      },
      "source": [
        "###Defining the PipeLine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUeolMd9SKHY",
        "outputId": "2578c4de-3cd7-4a59-8b8f-1ee399c421f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorShape([64, 24]), TensorShape([64, 24]))"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#sets up various configuration parameters and creates a TensorFlow dataset for training a machine learning model\n",
        "\n",
        "# length of the training input tensor (input_tensor_train). \n",
        "#This is often used to shuffle the dataset. In this case, the entire training dataset will be used for shuffling.\n",
        "BUFFER_SIZE = len(input_tensor_train)  #Shuffling, in the context of training a machine learning model, is the process of randomizing the order of the training data.\n",
        "\n",
        "BATCH_SIZE = 64 #It defines the number of training examples to be processed in each training batch.\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 400 \n",
        "#embedding_dim is set to 400. It specifies the dimensionality of the word embeddings. \n",
        "# Word embeddings are dense vector representations of words.\n",
        "units = 1500 # These units can represent the number of hidden units in a neural network layer.\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "#represent the vocabulary sizes for the input and target languages, respectively. \n",
        "# They are determined by the length of the word indices obtained from the tokenizers (inp_lang.word_index and targ_lang.word_index) and incremented by 1. \n",
        "# The addition of 1 is to account for a special token (usually <unk>) that may be added for unknown words.\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "#Creating a TensorFlow Dataset:\n",
        "#The tf.data.Dataset.from_tensor_slices function is used to create a dataset from the training data. \n",
        "# It combines the input and target tensors (sequences) into pairs.\n",
        "\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape\n",
        "\n",
        "#Example Batches:\n",
        "\n",
        "#example_input_batch and example_target_batch are obtained from the dataset using next(iter(dataset)). \n",
        "# These are example input and target batches for demonstration purposes.\n",
        "#Shapes:\n",
        "\n",
        "#The code prints the shapes of the example input and target batches using example_input_batch.shape and example_target_batch.shape. \n",
        "# This is often done to verify that the data has been properly batched.\n",
        "\n",
        "#his shape corresponds to the input batch of sequences.\n",
        "#  It's saying that you have a batch of 64 sequences, and each sequence has a length of 24."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Nk-cafrc3WM8"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model): # custom model class that inherits from tf.keras.Model\n",
        "  # The encoder's role is to process the input sequences and produce a fixed-length context representation.\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__() # custom model class that inherits from tf.keras.Model\n",
        "        self.batch_sz = batch_sz # batch size.\n",
        "        self.enc_units = enc_units # number of units (hidden dimensions) in the LSTM layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) #dimension of the word embeddings. converts input token IDs into dense word embeddings.\n",
        "        self.lstm = tf.keras.layers.LSTM(self.enc_units, #LSTM (Long Short-Term Memory) layer that processes the embedded input sequences.\n",
        "                                         return_sequences=True, # LSTM should return the full sequence of outputs for each time step.\n",
        "                                         return_state=True,  #indicates that the LSTM should return the final hidden state and cell state.\n",
        "                                         recurrent_initializer='glorot_uniform') #specifies the initializer for the recurrent weights.\n",
        "\n",
        "    def call(self, x, hidden): #defines the forward pass of the encoder.\n",
        "      x = self.embedding(x) # two arguments: x (the input sequences) and hidden (the initial hidden state and cell state of the LSTM).\n",
        "      output, state_h, state_c = self.lstm(x, initial_state=hidden)  \n",
        "      #input sequences are passed through the embedding layer and then through the LSTM layer.  \n",
        "      #The LSTM returns the output sequences, final hidden state (state_h), and final cell state (state_c).\n",
        "      state = [state_h, state_c] #The final state is stored in a list called state and returned along with the output sequences.\n",
        "      return output, state\n",
        "\n",
        "    def initialize_hidden_state(self): #initializes the hidden state and cell state with zeros. It's used to create the initial state for the LSTM layer.\n",
        "        return [tf.zeros((self.batch_sz, self.enc_units)),\n",
        "                tf.zeros((self.batch_sz, self.enc_units))]\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE) #Encoder Initialization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "adO6-QxYKlwx"
      },
      "outputs": [],
      "source": [
        "class Attention(tf.keras.layers.Layer): # custom layer in a machine learning model,inherits from tf.keras.layers.Layer.\n",
        "    def __init__(self, units):  #number of units (hidden dimensions) for the Dense layers within the attention mechanism.\n",
        "        super(Attention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units) \n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        #dense layers that are used to transform the input query and the values (typically encoder outputs) into compatible representations \n",
        "        # for calculating attention scores.\n",
        "        self.V = tf.keras.layers.Dense(1) #nother Dense layer used to produce the final attention scores.\n",
        "\n",
        "    def call(self, query, values): #defines the forward pass of the attention layer.\n",
        "        #query for which attention needs to be calculated. In sequence-to-sequence models, this is often a decoder hidden state.\n",
        "        #values represent the values to which attention is applied. In sequence-to-sequence models, this is often the encoder outputs.\n",
        "        query_with_time_axis = tf.expand_dims(query, 1) #expands the dimensions of the query to make it compatible with the values.\n",
        "        score = self.V(tf.nn.tanh( #alculates the attention scores using the tanh activation function and the weights from self.W1 and self.W2.\n",
        "        # The result is passed through the self.V layer to obtain final attention scores #!! YOU CAN MODIFY THE WAY SCORE IS CALCULATED. EG DOT PRODUCT\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1) #computed by applying the softmax function to the attention scores along axis=1. \n",
        "        #This produces a weight for each position in the values sequence, indicating the importance of each position.\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1) #calculated by multiplying the attention weights with the values and then summing along axis=1. I\n",
        "\n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ucM7JAxI4vM3"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):  #vocab_size is the size of the output vocabulary. embedding_dim is the dimension of the word embeddings for the decoder. \n",
        "    #dec_units is the number of units (hidden dimensions) in the LSTM layer of the decoder. batch_sz is the batch size.\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__() #custom model class that inherits from tf.keras.Model. \n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                         return_sequences=True,\n",
        "                                         return_state=True,\n",
        "                                         recurrent_initializer='glorot_uniform') #recurrent_initializer='glorot_uniform' specifies the initializer for the recurrent weights.\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size) #Dense layer that produces the final output predictions by transforming the LSTM output into the shape of the output vocabulary.\n",
        "        self.attention = Attention(self.dec_units) #Attention layer, which was defined earlier in your code. This is the attention mechanism that helps the decoder focus on relevant parts of the input sequence.\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        #x (the previous predicted token), hidden (the initial hidden state and cell state of the LSTM), and enc_output (the encoder's output sequences).\n",
        "        context_vector, attention_weights = self.attention(hidden[0], enc_output)\n",
        "\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Concatenate context vector and embedding\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # Passing the concatenated vector to the LSTM\n",
        "        output, state_h, state_c = self.lstm(x, initial_state=hidden)  # Use LSTM with state_h and state_c\n",
        "\n",
        "        state = [state_h, state_c]\n",
        "\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights\n",
        "\n",
        "# The attention mechanism is applied to calculate a context vector and attention weights.\n",
        "# The previous predicted token is passed through the embedding layer.\n",
        "# The context vector and embedded token are concatenated to create the input to the LSTM.\n",
        "# The LSTM processes this input and produces an output sequence, final hidden state (state_h), and final cell state (state_c).\n",
        "# The LSTM output is reshaped and passed through the Dense layer to produce the final output predictions.\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uHuaQpny3Wk"
      },
      "source": [
        "###Adjusting Learning Rates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Z0Z62qj-ZLru"
      },
      "outputs": [],
      "source": [
        "initial_learning_rate = 0.001 #adaptive learning rate\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay( #used to create a learning rate schedule. It's an exponential decay schedule that reduces the learning rate over time.\n",
        "    initial_learning_rate, decay_steps=1000, decay_rate=0.9, staircase=True #earning rate is reduced at discrete intervals (staircase decay).\n",
        ")\n",
        "\n",
        "# Define the optimizer with adaptive learning rate\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule) #optimizer will be used to update the model's weights during training.\n",
        "\n",
        "# Define your loss function\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none' #function expects the model's output to be unnormalized log probabilities (logits).  loss should not be reduced to a single scalar but kept as a per-sample loss.\n",
        ")\n",
        "\n",
        "def loss_function(real, pred):  #defining loss function\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "# It takes two arguments, real (the true target values) and pred (the model's predicted values).\n",
        "# A mask is created to ignore loss contributions from padding tokens (tokens with a value of 0).\n",
        "# The loss is calculated using loss_object and then multiplied by the mask to zero out the loss for padding tokens.\n",
        "# The final loss is obtained by computing the mean of the non-padded losses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXFR4CAxy9Od"
      },
      "source": [
        "###Defining Train Step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ftqY-Rj_XNlW"
      },
      "outputs": [],
      "source": [
        "@tf.function # defines a training step for the sequence-to-sequence model using TensorFlow. \n",
        "def train_step(inp, targ, enc_hidden): \n",
        "#inp: The input sequence.\n",
        "# targ: The target (output) sequence.\n",
        "# enc_hidden: The initial hidden state of the encoder.\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape: #tf.GradientTape is used to record operations for automatic differentiation, allowing the computation of gradients.\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "#The input sequence inp is passed through the encoder (encoder) to obtain enc_output (encoder outputs) and the final enc_hidden state.\n",
        "\n",
        "        # Initialize LSTM's initial state\n",
        "        dec_hidden = [enc_hidden[0][:, :units], enc_hidden[1][:, :units]]\n",
        "\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        #he initial hidden state of the decoder (dec_hidden) is initialized with a slice of enc_hidden to ensure compatibility with the decoder's LSTM layer.\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # Passing enc_output to the decoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            # Using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "            #Teacher forcing is employed, which means that during training, the model's predicted output at time step t is fed as input at time step t+1. \n",
        "            # This loop iterates over the target sequence length (targ.shape[1]).\n",
        "            # For each time step, the decoder (decoder) is called with the current dec_input, dec_hidden, and enc_output. \n",
        "            # This generates predictions for the next token in the sequence.\n",
        "            # The loss for the current time step is computed using the loss_function based on the predicted token and the actual target token. \n",
        "            # This loss is added to the loss variable.\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "    #calculated by dividing the accumulated loss by the length of the target sequence (targ.shape[1]). \n",
        "    # This provides the average loss per token in the target sequence for the current batch.\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    #gradients of the loss with respect to the model's trainable variables (encoder and decoder) are computed using the tape.gradient function.\n",
        "    #The optimizer is then used to apply these gradients to update the model's weights.\n",
        "    return batch_loss\n",
        "\n",
        "    # This train_step function encapsulates a single training step for the sequence-to-sequence model, including forward and backward passes. \n",
        "    # During training, this function is typically called within an epoch loop to train the model on batches of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lo-wQ2NPXOKm"
      },
      "outputs": [],
      "source": [
        "@tf.function #used to compute the validation loss for the sequence-to-sequence model during validation or evaluation\n",
        "def validation_step(inp, targ, enc_hidden):\n",
        "# The validation_step function takes three arguments:\n",
        "# inp: The input sequence.\n",
        "# targ: The target (output) sequence.\n",
        "# enc_hidden: The initial hidden state of the encoder.\n",
        "    val_loss = 0 #accumulate the validation loss.\n",
        "\n",
        "    val_samples = 0 #keep track of the number of validation samples.\n",
        "\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden) #Encoder Pass:\n",
        "\n",
        "#The input sequence inp is passed through the encoder (encoder) to obtain enc_output (encoder outputs) and the final enc_hidden state.\n",
        "\n",
        "    # Initialize LSTM's initial state\n",
        "    dec_hidden = [enc_hidden[0][:, :units], enc_hidden[1][:, :units]]\n",
        "    #initial hidden state of the decoder (dec_hidden) is initialized with a slice of enc_hidden to ensure compatibility with the decoder's LSTM layer.\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    for t in range(1, targ.shape[1]): #Teacher Forcing Loop for Validation:\n",
        "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "        loss = loss_function(targ[:, t], predictions)\n",
        "        val_loss += loss\n",
        "        val_samples += 1\n",
        "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    val_loss /= val_samples #Validation Loss and Samples:\n",
        "\n",
        "    return val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWKFyJ9ey__t"
      },
      "source": [
        "###Training the Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fEBjc3KTdgE-",
        "outputId": "73715586-df7f-4408-fcb4-a43dcf751531"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:  1 Loss:1.7199 Val Loss:1.7099\n",
            "Epoch:  2 Loss:1.5591 Val Loss:1.6226\n",
            "Epoch:  3 Loss:1.4329 Val Loss:1.5790\n",
            "Epoch:  4 Loss:1.3366 Val Loss:1.5512\n",
            "Epoch:  5 Loss:1.2278 Val Loss:1.5518\n",
            "Epoch:  6 Loss:1.1175 Val Loss:1.5585\n",
            "Epoch:  7 Loss:1.0044 Val Loss:1.5651\n",
            "Epoch:  8 Loss:0.8902 Val Loss:1.5885\n",
            "Epoch:  9 Loss:0.7825 Val Loss:1.6053\n",
            "Epoch: 10 Loss:0.6875 Val Loss:1.6335\n",
            "Epoch: 11 Loss:0.6072 Val Loss:1.6574\n",
            "Epoch: 12 Loss:0.5374 Val Loss:1.6918\n",
            "Epoch: 13 Loss:0.4852 Val Loss:1.7324\n",
            "Epoch: 14 Loss:0.4775 Val Loss:1.7416\n",
            "Epoch: 15 Loss:0.4293 Val Loss:1.7674\n",
            "Epoch: 16 Loss:0.4035 Val Loss:1.7962\n",
            "Epoch: 17 Loss:0.3826 Val Loss:1.8100\n",
            "Epoch: 18 Loss:0.3690 Val Loss:1.8319\n",
            "Epoch: 19 Loss:0.3513 Val Loss:1.8428\n",
            "Epoch: 20 Loss:0.3385 Val Loss:1.8569\n",
            "Epoch: 21 Loss:0.3272 Val Loss:1.8767\n",
            "Epoch: 22 Loss:0.3181 Val Loss:1.8974\n",
            "Epoch: 23 Loss:0.3097 Val Loss:1.9002\n",
            "Epoch: 24 Loss:0.2999 Val Loss:1.9210\n",
            "Epoch: 25 Loss:0.2907 Val Loss:1.9264\n",
            "Epoch: 26 Loss:0.2816 Val Loss:1.9481\n",
            "Epoch: 27 Loss:0.2729 Val Loss:1.9581\n",
            "Epoch: 28 Loss:0.2639 Val Loss:1.9698\n"
          ]
        }
      ],
      "source": [
        "#represents the training loop for your sequence-to-sequence model and \n",
        "# visualizes the training and validation losses over a specified number of epochs (EPOCHS).\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "EPOCHS = 60\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    enc_hidden = encoder.initialize_hidden_state() #initializes the hidden state for the encoder (enc_hidden) using the encoder.initialize_hidden_state() function.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Training loop\n",
        "    #Batch Training Loop:\n",
        "\n",
        "# Within each epoch, there's an inner loop that iterates over the training dataset (dataset) for each batch.\n",
        "# In each batch, it calls the train_step function to compute the training loss for the current batch. \n",
        "# The train_step function handles forward and backward passes for training.\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "    num_samples = 0\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "      enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "      dec_hidden = enc_hidden\n",
        "      dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "      for t in range(1, targ.shape[1]):\n",
        "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "        predicted_id = tf.argmax(predictions, axis=-1)\n",
        "        num_samples += 1\n",
        "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    validation_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n",
        "    validation_dataset = validation_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "#After training for one epoch, a separate validation loop is performed to evaluate the model's performance on the validation dataset.\n",
        "# It initializes the hidden state for the encoder (enc_hidden) for each batch in the validation dataset.\n",
        "# For each batch, it calls the validation_step function to compute the validation loss for the current batch.\n",
        "    val_loss = 0\n",
        "    val_samples = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(validation_dataset):\n",
        "      enc_hidden = encoder.initialize_hidden_state()  # Initialize hidden state for each batch\n",
        "      val_batch_loss = validation_step(inp, targ, enc_hidden)\n",
        "      val_loss += val_batch_loss\n",
        "      val_samples += 1\n",
        "\n",
        "    val_loss /= val_samples\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        train_losses.append(total_loss / steps_per_epoch)\n",
        "        val_losses.append(val_loss)\n",
        "        print('Epoch:{:3d} Loss:{:.4f} Val Loss:{:.4f}'.format(\n",
        "            epoch, total_loss / steps_per_epoch,  val_loss))\n",
        "\n",
        "# Plotting the accuracy and loss graphs\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot training and validation losses\n",
        "plt.plot(range(1, EPOCHS + 1), train_losses, label='Train')\n",
        "plt.plot(range(1, EPOCHS + 1), val_losses, label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#training took 3 hours to complete with colab GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c2YfFId3U5Y",
        "outputId": "9898c4b4-4df7-428d-a985-836d58553552"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) args_1 with unsupported characters which will be renamed to args_1_1 in the SavedModel.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Function `_wrapped_model` contains input name(s) args_1 with unsupported characters which will be renamed to args_1_1 in the SavedModel.\n",
            "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, dense_4_layer_call_fn, dense_4_layer_call_and_return_conditional_losses, dense_5_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n",
            "WARNING:absl:<__main__.Attention object at 0x7ff671ded3f0> has the same name 'Attention' as a built-in Keras object. Consider renaming <class '__main__.Attention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ],
      "source": [
        "encoder.save(\"encoder_final\")\n",
        "decoder.save(\"decoder_final\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12f5dE5E3XZ-",
        "outputId": "1990d3cf-3808-46d9-d857-b5977b77f83e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: encoder_final/ (stored 0%)\n",
            "  adding: encoder_final/variables/ (stored 0%)\n",
            "  adding: encoder_final/variables/variables.index (deflated 39%)\n",
            "  adding: encoder_final/variables/variables.data-00000-of-00001 (deflated 7%)\n",
            "  adding: encoder_final/keras_metadata.pb (deflated 80%)\n",
            "  adding: encoder_final/fingerprint.pb (stored 0%)\n",
            "  adding: encoder_final/saved_model.pb (deflated 91%)\n",
            "  adding: encoder_final/assets/ (stored 0%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r \"encoder_final.zip\" \"encoder_final\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14lbjgIC3acq",
        "outputId": "db133072-6631-413a-d259-251ad41d4eda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: decoder_final/ (stored 0%)\n",
            "  adding: decoder_final/variables/ (stored 0%)\n",
            "  adding: decoder_final/variables/variables.index (deflated 52%)\n",
            "  adding: decoder_final/variables/variables.data-00000-of-00001 (deflated 7%)\n",
            "  adding: decoder_final/keras_metadata.pb (deflated 87%)\n",
            "  adding: decoder_final/fingerprint.pb (stored 0%)\n",
            "  adding: decoder_final/saved_model.pb (deflated 90%)\n",
            "  adding: decoder_final/assets/ (stored 0%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r \"decoder_final.zip\" \"decoder_final\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gFtSqTElXltk"
      },
      "outputs": [],
      "source": [
        "def remove_tags(sentence):\n",
        "    return sentence.split(\"<start>\")[-1].split(\"<end>\")[0] #utility function that removes the \"<start>\" and \"<end>\" tags from a sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "w3qbBcT6IPkf"
      },
      "outputs": [],
      "source": [
        "def evaluate(sentence): #function takes an input sentence as its parameter.\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')] #tokenized into a sequence of word indices using the inp_lang.word_index.\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], #inp_lang.word_index is a dictionary where words are mapped to their corresponding numerical indices. \n",
        "                                                                                                                #For example, if you have a dictionary like {'hello': 1, 'how': 2, 'are': 3, ...}, \n",
        "                                                                                                                # it means that \"hello\" is represented by the index 1, \"how\" by 2, and so on.\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post') \n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = '' #This variable will store the generated response.\n",
        "\n",
        "    hidden = [tf.zeros((1, units)), tf.zeros((1, units))] #Initial hidden states are created as [tf.zeros((1, units)), tf.zeros((1, units))].\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)#nput to the decoder is initialized with the \"<start>\" token.\n",
        " \n",
        "    for t in range(max_length_targ): #Loop for Generating Output: iterates for a maximum of max_length_targ times. This is the maximum length for the target sequence.\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return remove_tags(result), remove_tags(sentence)\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "#In each iteration:\n",
        "# The decoder is called with the current dec_input, dec_hidden, and enc_out.\n",
        "# The model generates predictions for the next token in the sequence.\n",
        "# The predicted token with the highest probability is selected using tf.argmax.\n",
        "# The predicted token is appended to the result.\n",
        "# If the predicted token is \"<end>\", the generation process stops.\n",
        "# Otherwise, the predicted token is used as the next input to the decoder for the next iteration.\n",
        "    return remove_tags(result), remove_tags(sentence)\n",
        "#the content of the result, and the preprocessed input sentence without tags (tags are removed using remove_tags)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing some random questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG40LpHdONCG",
        "outputId": "bc298204-fd4f-4d08-9b2e-cc0222eedcd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  good luck with school \n",
            "Predicted answer: thank you very much . \n"
          ]
        }
      ],
      "source": [
        "def test(question):\n",
        "    answer, question = evaluate(question)\n",
        "    print('Question:', question)\n",
        "    print('Predicted answer:', answer)\n",
        "\n",
        "test(\"good luck with school\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99zBE0TPZ8WG",
        "outputId": "dd68980d-b4e5-4fb1-8a8d-87f303e5c5e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  hello \n",
            "Predicted answer: greetings ! \n",
            "Question:  how are you doing ? \n",
            "Predicted answer: fine , and you ? \n",
            "Question:  what is your age ? \n",
            "Predicted answer: i am still young by your standards . \n",
            "Question:  do you have a tv ? \n",
            "Predicted answer: yes , i do . \n",
            "Question:  do you like rain ? \n",
            "Predicted answer: yes , i love traveling and exploring new places . \n"
          ]
        }
      ],
      "source": [
        "test(\"Hello\")\n",
        "test(\"How are you doing?\")\n",
        "test(\"What is your age?\")\n",
        "test(\"Do you have a tv?\")\n",
        "test(\"Do you like rain?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvz7kNlj25-K",
        "outputId": "4e32399c-d419-40c7-a278-e350139045c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  i am afraid \n",
            "Predicted answer: why ? do i frighten you ? try not to be too scared . what are you afraid of ? \n"
          ]
        }
      ],
      "source": [
        "test(\"I am afraid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJoKOlrM28e9",
        "outputId": "f4a5539c-0924-4be7-fc7c-d12c3d10a299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  i am feeling sick \n",
            "Predicted answer: oh , really ? \n"
          ]
        }
      ],
      "source": [
        "test(\"I am feeling sick\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCDxO6fiNyjr",
        "outputId": "3c971737-1215-4dba-c5d0-e6648372b41d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  sorry \n",
            "Predicted answer: yeah , so do i . \n"
          ]
        }
      ],
      "source": [
        "test(\"Sorry\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11vTxcJRN0AS",
        "outputId": "453c65bd-87c2-4325-9acf-667197149c2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  hi , how are you doing ? \n",
            "Predicted answer: i m fine . how about yourself ? \n"
          ]
        }
      ],
      "source": [
        "test(\"hi, how are you doing?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGgZlEWfN5sG",
        "outputId": "77507157-5750-48eb-da97-0969e0489d3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  i m pretty good . thanks for asking . \n",
            "Predicted answer: no problem . so how have you been ? \n"
          ]
        }
      ],
      "source": [
        "test(\"i'm pretty good. thanks for asking.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61qs3yLVN8Ko",
        "outputId": "7cd07063-8f05-4f72-8f38-36118072cf43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  i ve been great . what about you ? \n",
            "Predicted answer: i ve been good . i m in school right now . \n"
          ]
        }
      ],
      "source": [
        "test(\"i've been great. what about you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTylB062N_qd",
        "outputId": "687e05c3-e014-4ef9-f0c3-ce1f642470f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  what school do you go to ? \n",
            "Predicted answer: i go to pcc . \n"
          ]
        }
      ],
      "source": [
        "test(\"what school do you go to?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ3xo_rNOEwB",
        "outputId": "5321db08-bfce-4614-8398-80a4908cc254"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  i don t know \n",
            "Predicted answer: i like the ones i can sing along with . \n"
          ]
        }
      ],
      "source": [
        "test(\"I don't know\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvaXsRBMOGk9",
        "outputId": "810c7d02-4152-4603-9900-6b3cf4504e2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  nice to meet you \n",
            "Predicted answer: thank you . \n"
          ]
        }
      ],
      "source": [
        "test(\"nice to meet you\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOrbnn6zOJvj",
        "outputId": "8ce3c076-7169-4fc8-99e8-0d046f0bf41f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  what are your hobbies ? \n",
            "Predicted answer: i enjoy reading books and playing the guitar . \n"
          ]
        }
      ],
      "source": [
        "test(\"What are your hobbies?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le5qUCLbOMAF",
        "outputId": "eaf250ac-7a94-44df-f01f-f38fd8ccba0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  you are rude \n",
            "Predicted answer: yep . i always behave in socially unacceptable ways . \n"
          ]
        }
      ],
      "source": [
        "test(\"You are rude\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKqioan4ONn6",
        "outputId": "bf2f7666-1e59-4d20-d7e0-3f52b7642441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:  i love you \n",
            "Predicted answer: i love you , too . \n"
          ]
        }
      ],
      "source": [
        "test(\"I love you\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eB1yEq__OTEw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
